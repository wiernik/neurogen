# A. Applying [VCMNet](https://github.com/LAAC-LSCP/vcm)

1. First, we need to extract KCHI utterances from annotation files:

```sh
python utils/extract_rttm_KCHI.py --data data/L3_HIPAA_LENA_cleaned/annotations/eaf/an1/converted --output data/L3_HIPAA_LENA_cleaned/annotations/eaf/an1/rttm
```

This will create a folder containing .rttm files with KCHI utterances such as found by the human annotator. 

2. We can then apply VCM following the instructions in this [git](https://github.com/LAAC-LSCP/vcm).

# B. Applying [Meg's algorithm](https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM)

Documentation for this model can be found [here](https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM).

1. Install the dependencies:

```shell
git clone https://github.com/spoglab-stanford/w2v2-pro-sm
cd w2v2-pro-sm/speechbrain/recipes/W2V2-LL4300-Pro-SM
conda env create -f environment.yaml
conlda activate megvcm
cd ../..
pip install -e . # Install Theo's speechbrain fork
conda install git-lfs
```

2. Extract KCHI utterances from .csv files

```shell
python utils/childproject_extract_KCHI.py --project data/L3_HIPAA_LENA_cleaned --set eaf/an1
```

This will create `samples/chi/segments_all_chi_eaf_an1.csv` containing all KCHI utterances such as identified by our human annotator.

3. From within the L3_HIPAA_LENA_cleaned, you can splice audio KCHI utterances into 500ms segments (since the model is working with fixed-length segments) using the following command. 
Note that you must run this step in oberon since this is where the audio files are stored. 

```shell
child-project zooniverse extract-chunks . --keyword maturity --chunks-length 500 --segments samples/chi/segments_all_chi_eaf_an1.csv --destination chunks/eaf_an1 --threads 16
```

4. Convert the .csv file generated by child-project into .json files:

```shell
python utils/csv_to_megvcm_json.py --input /scratch1/data/raw_data/neurogen/L3_HIPAA_LENA_cleaned/chunks/eaf_an1/chunks_20250613_205715.csv
```

5. From within the `W2V2-LL4300-Pro-SM` folder, create the directory where predictions will be saved.

```shell
mkdir -p 2025/myst_checkpoints
cd 2025/myst_checkpoints
git clone https://huggingface.co/spoglab/w2v2-pro-sm
cd w2v2-pro-sm
git lfs install
git lfs pull
cd ..
mv w2v2-pro-sm/* .
mv checkpoint_best.pt ../..
rm -rf w2v2-pro-sm
```

6. Update all the paths and seed in `hparams/train_1_w2v2_2dnn_WA_LL4300_asr_bbcor_concat.yaml` (seed,output_folder,train_annotation,valid_annotation,test_annotation,wav2vec2.pretrained_path) and `train_1_w2v2_WA_2dnn_combine_asr_features_bbcor.py` (2 changes)


7. Run the model using:

```shell
python train_1_w2v2_WA_2dnn_combine_asr_features_bbcor.py hparams/train_1_w2v2_2dnn_WA_LL4300_asr_bbcor_concat.yaml
```

8. Create child-project files based on the model's output:

```shell
python utils/convert_megvcm_predictions.py --predictions /path/to/L3_HIPAA_LENA_cleaned/chunks/eaf_an1/megvcm_predictions_eaf_an1.csv --chunks /path/to/L3_HIPAA_LENA_cleaned/chunks/eaf_an1/chunks_20250613_205715.csv --project /path/to/L3_HIPAA_LENA_cleaned --set eaf/an1
```